{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practica para Sistemas de Bases de Datos (Feb 21)\n",
    "\n",
    "## Autor: Martín Romera Sobrado\n",
    "## Contacto: mromera95@alumno.uned.es"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inicialización de la base de datos\n",
    "\n",
    "Creamos e iniciamos la base de datos si todavía no lo está."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! hadoop fs -mkdir librosdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! hadoop fs -mkdir librosdb/authors/ && hadoop fs -mkdir librosdb/dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scan complete in 21ms\n",
      "Connecting to jdbc:hive2://localhost:10000/default\n",
      "Connected to: Apache Hive (version 1.1.0-cdh5.12.0)\n",
      "Driver: Hive JDBC (version 1.1.0-cdh5.12.0)\n",
      "Transaction isolation: TRANSACTION_REPEATABLE_READ\n",
      "que no estee2://localhost:10000/default> -- Crea la base de datos en el caso de   creada.\n",
      "0: jdbc:hive2://localhost:10000/default> \n",
      "0: jdbc:hive2://localhost:10000/default> create database if not exists librosdb\n",
      "de libros'. . . . . . . . . . . . . . .> Comment 'Base de datos con información  \n",
      ". . . . . . . . . . . . . . . . . . . .> Location '/home/cloudera/librosdb'\n",
      "n Romera Sob. . . . . . . . . . . . . .> With dbproperties('Creada por' = 'Martí rado', 'Creada el' = '9-Nov-2020');\n",
      "INFO  : Compiling command(queryId=hive_20201123093232_637f1ac4-052c-41a0-a13d-4ec7d71cebf1): create database if not exists librosdb\n",
      "Comment 'Base de datos con información de libros'\n",
      "Location '/home/cloudera/librosdb'\n",
      "With dbproperties('Creada por' = 'Martín Romera Sobrado', 'Creada el' = '9-Nov-2020')\n",
      "INFO  : Semantic Analysis Completed\n",
      "INFO  : Returning Hive schema: Schema(fieldSchemas:null, properties:null)\n",
      "INFO  : Completed compiling command(queryId=hive_20201123093232_637f1ac4-052c-41a0-a13d-4ec7d71cebf1); Time taken: 0.66 seconds\n",
      "INFO  : Concurrency mode is disabled, not creating a lock manager\n",
      "INFO  : Executing command(queryId=hive_20201123093232_637f1ac4-052c-41a0-a13d-4ec7d71cebf1): create database if not exists librosdb\n",
      "Comment 'Base de datos con información de libros'\n",
      "Location '/home/cloudera/librosdb'\n",
      "With dbproperties('Creada por' = 'Martín Romera Sobrado', 'Creada el' = '9-Nov-2020')\n",
      "INFO  : Starting task [Stage-0:DDL] in serial mode\n",
      "INFO  : Completed executing command(queryId=hive_20201123093232_637f1ac4-052c-41a0-a13d-4ec7d71cebf1); Time taken: 1.43 seconds\n",
      "INFO  : OK\n",
      "No rows affected (2.38 seconds)\n",
      "0: jdbc:hive2://localhost:10000/default> \n",
      "0: jdbc:hive2://localhost:10000/default> \n",
      "Closing: 0: jdbc:hive2://localhost:10000/default\n"
     ]
    }
   ],
   "source": [
    "! beeline -u \"jdbc:hive2://localhost:10000/default\" -f db/db.hql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los contenidos de *db/db.hql* son los siguientes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```HiveQL\n",
    "-- Crea la base de datos en el caso de que no este creada.\n",
    "\n",
    "create database if not exists librosdb\n",
    "Comment 'Base de datos con información de libros'\n",
    "Location '/home/cloudera/librosdb'\n",
    "With dbproperties('Creada por' = 'Martín Romera Sobrado', 'Creada el' = '9-Nov-2020');\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ej 1. *Crea las tablas necesarias para almacenar los datos. Pueden ser internas o externas en función de los datos que se desee. La decisión de interna o externa debe estar razonada.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las tablas serán externas, ya que en el ejercicio 5 tendremos un programa *MapReduce* en python que acceda a los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scan complete in 3ms\n",
      "Connecting to jdbc:hive2://localhost:10000/\n",
      "Connected to: Apache Hive (version 1.1.0-cdh5.12.0)\n",
      "Driver: Hive JDBC (version 1.1.0-cdh5.12.0)\n",
      "Transaction isolation: TRANSACTION_REPEATABLE_READ\n",
      "0: jdbc:hive2://localhost:10000/> -- Creamos las dos tablas: authors y datasets\n",
      "0: jdbc:hive2://localhost:10000/> \n",
      "0: jdbc:hive2://localhost:10000/> USE librosdb;\n",
      "INFO  : Compiling command(queryId=hive_20201123093232_b8cf63ad-e509-4107-aeae-83444ccd3384): USE librosdb\n",
      "INFO  : Semantic Analysis Completed\n",
      "INFO  : Returning Hive schema: Schema(fieldSchemas:null, properties:null)\n",
      "INFO  : Completed compiling command(queryId=hive_20201123093232_b8cf63ad-e509-4107-aeae-83444ccd3384); Time taken: 0.065 seconds\n",
      "INFO  : Concurrency mode is disabled, not creating a lock manager\n",
      "INFO  : Executing command(queryId=hive_20201123093232_b8cf63ad-e509-4107-aeae-83444ccd3384): USE librosdb\n",
      "INFO  : Starting task [Stage-0:DDL] in serial mode\n",
      "INFO  : Completed executing command(queryId=hive_20201123093232_b8cf63ad-e509-4107-aeae-83444ccd3384); Time taken: 0.011 seconds\n",
      "INFO  : OK\n",
      "No rows affected (0.128 seconds)\n",
      "0: jdbc:hive2://localhost:10000/> \n",
      "0: jdbc:hive2://localhost:10000/> -- Tabla authors\n",
      "0: jdbc:hive2://localhost:10000/> \n",
      "0: jdbc:hive2://localhost:10000/> CREATE EXTERNAL TABLE IF NOT EXISTS authors\n",
      ". . . . . . . . . . . . . . . . > (\n",
      ". . . . . . . . . . . . . . . . >     author_id INT,\n",
      ". . . . . . . . . . . . . . . . >     author_name STRING\n",
      ". . . . . . . . . . . . . . . . > )\n",
      ". . . . . . . . . . . . . . . . > COMMENT 'Tabla de autores'\n",
      ". . . . . . . . . . . . . . . . > ROW FORMAT DELIMITED\n",
      ". . . . . . . . . . . . . . . . > FIELDS TERMINATED BY '\\;'\n",
      ". . . . . . . . . . . . . . . . > tblproperties(\"skip.header.line.count\"=\"1\"); \n",
      "INFO  : Compiling command(queryId=hive_20201123093232_f179379c-5cfa-48cb-bbb1-262107f0aa56): CREATE EXTERNAL TABLE IF NOT EXISTS authors\n",
      "(\n",
      "author_id INT,\n",
      "author_name STRING\n",
      ")\n",
      "COMMENT 'Tabla de autores'\n",
      "ROW FORMAT DELIMITED\n",
      "FIELDS TERMINATED BY ';'\n",
      "tblproperties(\"skip.header.line.count\"=\"1\")\n",
      "INFO  : Semantic Analysis Completed\n",
      "INFO  : Returning Hive schema: Schema(fieldSchemas:null, properties:null)\n",
      "INFO  : Completed compiling command(queryId=hive_20201123093232_f179379c-5cfa-48cb-bbb1-262107f0aa56); Time taken: 0.173 seconds\n",
      "INFO  : Concurrency mode is disabled, not creating a lock manager\n",
      "INFO  : Executing command(queryId=hive_20201123093232_f179379c-5cfa-48cb-bbb1-262107f0aa56): CREATE EXTERNAL TABLE IF NOT EXISTS authors\n",
      "(\n",
      "author_id INT,\n",
      "author_name STRING\n",
      ")\n",
      "COMMENT 'Tabla de autores'\n",
      "ROW FORMAT DELIMITED\n",
      "FIELDS TERMINATED BY ';'\n",
      "tblproperties(\"skip.header.line.count\"=\"1\")\n",
      "INFO  : Starting task [Stage-0:DDL] in serial mode\n",
      "INFO  : Completed executing command(queryId=hive_20201123093232_f179379c-5cfa-48cb-bbb1-262107f0aa56); Time taken: 0.262 seconds\n",
      "INFO  : OK\n",
      "No rows affected (0.449 seconds)\n",
      "0: jdbc:hive2://localhost:10000/> \n",
      "0: jdbc:hive2://localhost:10000/> -- Tabla datasets\n",
      "0: jdbc:hive2://localhost:10000/> \n",
      "0: jdbc:hive2://localhost:10000/> CREATE EXTERNAL TABLE IF NOT EXISTS datasets\n",
      ". . . . . . . . . . . . . . . . > (\n",
      ". . . . . . . . . . . . . . . . >     title STRING,\n",
      ". . . . . . . . . . . . . . . . >     author_id INT,\n",
      ". . . . . . . . . . . . . . . . >     bestsellers_rank INT,\n",
      ". . . . . . . . . . . . . . . . >     imprint BINARY,\n",
      ". . . . . . . . . . . . . . . . >     publication_date STRING,\n",
      ". . . . . . . . . . . . . . . . >     rating_avg DOUBLE,\n",
      ". . . . . . . . . . . . . . . . >     rating_count INT\n",
      ". . . . . . . . . . . . . . . . > )\n",
      ". . . . . . . . . . . . . . . . > COMMENT 'Tabla de libros'\n",
      ". . . . . . . . . . . . . . . . > ROW FORMAT DELIMITED\n",
      ". . . . . . . . . . . . . . . . > FIELDS TERMINATED BY '\\;'\n",
      ". . . . . . . . . . . . . . . . > tblproperties(\"skip.header.line.count\"=\"1\"); \n",
      "INFO  : Compiling command(queryId=hive_20201123093232_52300499-d71f-4db8-92d3-b64f09da2aa0): CREATE EXTERNAL TABLE IF NOT EXISTS datasets\n",
      "(\n",
      "title STRING,\n",
      "author_id INT,\n",
      "bestsellers_rank INT,\n",
      "imprint BINARY,\n",
      "publication_date STRING,\n",
      "rating_avg DOUBLE,\n",
      "rating_count INT\n",
      ")\n",
      "COMMENT 'Tabla de libros'\n",
      "ROW FORMAT DELIMITED\n",
      "FIELDS TERMINATED BY ';'\n",
      "tblproperties(\"skip.header.line.count\"=\"1\")\n",
      "INFO  : Semantic Analysis Completed\n",
      "INFO  : Returning Hive schema: Schema(fieldSchemas:null, properties:null)\n",
      "INFO  : Completed compiling command(queryId=hive_20201123093232_52300499-d71f-4db8-92d3-b64f09da2aa0); Time taken: 0.042 seconds\n",
      "INFO  : Concurrency mode is disabled, not creating a lock manager\n",
      "INFO  : Executing command(queryId=hive_20201123093232_52300499-d71f-4db8-92d3-b64f09da2aa0): CREATE EXTERNAL TABLE IF NOT EXISTS datasets\n",
      "(\n",
      "title STRING,\n",
      "author_id INT,\n",
      "bestsellers_rank INT,\n",
      "imprint BINARY,\n",
      "publication_date STRING,\n",
      "rating_avg DOUBLE,\n",
      "rating_count INT\n",
      ")\n",
      "COMMENT 'Tabla de libros'\n",
      "ROW FORMAT DELIMITED\n",
      "FIELDS TERMINATED BY ';'\n",
      "tblproperties(\"skip.header.line.count\"=\"1\")\n",
      "INFO  : Starting task [Stage-0:DDL] in serial mode\n",
      "INFO  : Completed executing command(queryId=hive_20201123093232_52300499-d71f-4db8-92d3-b64f09da2aa0); Time taken: 0.07 seconds\n",
      "INFO  : OK\n",
      "No rows affected (0.121 seconds)\n",
      "0: jdbc:hive2://localhost:10000/> \n",
      "0: jdbc:hive2://localhost:10000/> \n",
      "0: jdbc:hive2://localhost:10000/> \n",
      "Closing: 0: jdbc:hive2://localhost:10000/\n"
     ]
    }
   ],
   "source": [
    "! beeline -u \"jdbc:hive2://localhost:10000/\" -f db/ej1.hql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los contenidos de *db/ej1.hql* son los siquientes:\n",
    "```HiveQL\n",
    "-- Creamos las dos tablas: authors y datasets\n",
    "\n",
    "USE librosdb;\n",
    "\n",
    "-- Tabla authors\n",
    "\n",
    "CREATE EXTERNAL TABLE IF NOT EXISTS authors\n",
    "(\n",
    "    author_id INT,\n",
    "    author_name STRING\n",
    ")\n",
    "COMMENT 'Tabla de autores';\n",
    "\n",
    "-- Tabla datasets\n",
    "\n",
    "CREATE EXTERNAL TABLE IF NOT EXISTS datasets\n",
    "(\n",
    "    title STRING,\n",
    "    author_id INT,\n",
    "    bestsellers_rank INT,\n",
    "    imprint BINARY,\n",
    "    publication_date TIMESTAMP,\n",
    "    rating_avg STRING,\n",
    "    rating_count INT\n",
    ")\n",
    "COMMENT 'Tabla de libros'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ej 2. *Importa los datos en las tablas creadas*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! hadoop fs -put db/csv/authors.csv librosdb/authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! hadoop fs -put db/csv/dataset.csv librosdb/dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scan complete in 3ms\n",
      "Connecting to jdbc:hive2://localhost:10000/\n",
      "Connected to: Apache Hive (version 1.1.0-cdh5.12.0)\n",
      "Driver: Hive JDBC (version 1.1.0-cdh5.12.0)\n",
      "Transaction isolation: TRANSACTION_REPEATABLE_READ\n",
      "tosjdbc:hive2://localhost:10000/> -- Importamos datos de los csv a la base de da \n",
      "0: jdbc:hive2://localhost:10000/> \n",
      "0: jdbc:hive2://localhost:10000/> USE librosdb;\n",
      "INFO  : Compiling command(queryId=hive_20201123093232_88c3990e-c06e-4481-a80a-8fa9051217a3): USE librosdb\n",
      "INFO  : Semantic Analysis Completed\n",
      "INFO  : Returning Hive schema: Schema(fieldSchemas:null, properties:null)\n",
      "INFO  : Completed compiling command(queryId=hive_20201123093232_88c3990e-c06e-4481-a80a-8fa9051217a3); Time taken: 0.065 seconds\n",
      "INFO  : Concurrency mode is disabled, not creating a lock manager\n",
      "INFO  : Executing command(queryId=hive_20201123093232_88c3990e-c06e-4481-a80a-8fa9051217a3): USE librosdb\n",
      "INFO  : Starting task [Stage-0:DDL] in serial mode\n",
      "INFO  : Completed executing command(queryId=hive_20201123093232_88c3990e-c06e-4481-a80a-8fa9051217a3); Time taken: 0.012 seconds\n",
      "INFO  : OK\n",
      "No rows affected (0.124 seconds)\n",
      "0: jdbc:hive2://localhost:10000/> \n",
      "ors/authors.csv' INTO TABLE authors;AD DATA INPATH '/user/cloudera/librosdb/auth \n",
      "INFO  : Compiling command(queryId=hive_20201123093232_0585f98a-ed7e-4148-b6d7-be65f69abe2f): LOAD DATA INPATH '/user/cloudera/librosdb/authors/authors.csv' INTO TABLE authors\n",
      "INFO  : Semantic Analysis Completed\n",
      "INFO  : Returning Hive schema: Schema(fieldSchemas:null, properties:null)\n",
      "INFO  : Completed compiling command(queryId=hive_20201123093232_0585f98a-ed7e-4148-b6d7-be65f69abe2f); Time taken: 0.062 seconds\n",
      "INFO  : Concurrency mode is disabled, not creating a lock manager\n",
      "INFO  : Executing command(queryId=hive_20201123093232_0585f98a-ed7e-4148-b6d7-be65f69abe2f): LOAD DATA INPATH '/user/cloudera/librosdb/authors/authors.csv' INTO TABLE authors\n",
      "INFO  : Starting task [Stage-0:MOVE] in serial mode\n",
      "INFO  : Loading data to table librosdb.authors from hdfs://quickstart.cloudera:8020/user/cloudera/librosdb/authors/authors.csv\n",
      "INFO  : Starting task [Stage-1:STATS] in serial mode\n",
      "INFO  : Table librosdb.authors stats: [numFiles=1, totalSize=464036]\n",
      "INFO  : Completed executing command(queryId=hive_20201123093232_0585f98a-ed7e-4148-b6d7-be65f69abe2f); Time taken: 0.52 seconds\n",
      "INFO  : OK\n",
      "No rows affected (0.596 seconds)\n",
      "0: jdbc:hive2://localhost:10000/> \n",
      "set/dataset.csv' INTO TABLE dataset;AD DATA INPATH '/user/cloudera/librosdb/data \n",
      "Error: Error while compiling statement: FAILED: SemanticException [Error 10001]: Line 1:74 Table not found 'dataset' (state=42S02,code=10001)\n",
      "\n",
      "Closing: 0: jdbc:hive2://localhost:10000/\n"
     ]
    }
   ],
   "source": [
    "! beeline -u \"jdbc:hive2://localhost:10000/\" -f db/ej2.hql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los contenidos de el archivo *db/ej2.hql*, son los siguientes:\n",
    "\n",
    "```HiveQL\n",
    "-- Importamos datos de los csv a la base de datos\n",
    "\n",
    "USE librosdb;\n",
    "\n",
    "LOAD DATA INPATH '/user/cloudera/librosdb/authors.csv' INTO TABLE authors;\n",
    "\n",
    "LOAD DATA INPATH '/user/cloudera/librosdb/dataset.csv' INTO TABLE datasets;\n",
    "```\n",
    "\n",
    "Los dos ficheros csv, son los que proporcionan para la práctica, sin se alterados en ningún momento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ej 3. *Crea una vista sobre las tablas creadas. Esta vista tendrá para cada título, el nombre del autor, fecha de publicación, y valoración media.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scan complete in 3ms\n",
      "Connecting to jdbc:hive2://localhost:10000/\n",
      "Connected to: Apache Hive (version 1.1.0-cdh5.12.0)\n",
      "Driver: Hive JDBC (version 1.1.0-cdh5.12.0)\n",
      "Transaction isolation: TRANSACTION_REPEATABLE_READ\n",
      "los siguientes atributos::10000/> -- Crea una vista para las tablas creadas con  \n",
      "ión y Valoración mediaost:10000/> -- Titulo, Nombre del autor, Fecha de publicac \n",
      "0: jdbc:hive2://localhost:10000/> USE librosdb;\n",
      "INFO  : Compiling command(queryId=hive_20201123093232_608d8175-2f7f-478b-b544-35e45d4ca5f1): USE librosdb\n",
      "INFO  : Semantic Analysis Completed\n",
      "INFO  : Returning Hive schema: Schema(fieldSchemas:null, properties:null)\n",
      "INFO  : Completed compiling command(queryId=hive_20201123093232_608d8175-2f7f-478b-b544-35e45d4ca5f1); Time taken: 0.062 seconds\n",
      "INFO  : Concurrency mode is disabled, not creating a lock manager\n",
      "INFO  : Executing command(queryId=hive_20201123093232_608d8175-2f7f-478b-b544-35e45d4ca5f1): USE librosdb\n",
      "INFO  : Starting task [Stage-0:DDL] in serial mode\n",
      "INFO  : Completed executing command(queryId=hive_20201123093232_608d8175-2f7f-478b-b544-35e45d4ca5f1); Time taken: 0.01 seconds\n",
      "INFO  : OK\n",
      "No rows affected (0.118 seconds)\n",
      "0: jdbc:hive2://localhost:10000/> \n",
      "0: jdbc:hive2://localhost:10000/> CREATE VIEW IF NOT EXISTS vista_libros \n",
      ". . . . . . . . . . . . . . . . > (\n",
      ". . . . . . . . . . . . . . . . >     title COMMENT 'Titulo del libro',\n",
      ". . . . . . . . . . . . . . . . >     author_name COMMENT 'Autor del libro',\n",
      "ció . . . . . . . . . . . . . . >     publication_date COMMENT 'Fecha de publica n',\n",
      "ibro' . . . . . . . . . . . . . >     rating_avg COMMENT 'Valoración media del l \n",
      ". . . . . . . . . . . . . . . . > )\n",
      ", rating_avg. . . . . . . . . . > AS SELECT title, author_name, publication_date \n",
      ". . . . . . . . . . . . . . . . > FROM authors, dataset\n",
      ". . . . . . . . . . . . . . . . > WHERE authors.author_id = dataset.author_id;\n",
      "Error: Error while compiling statement: FAILED: SemanticException [Error 10001]: Line 9:14 Table not found 'dataset' (state=42S02,code=10001)\n",
      "\n",
      "Closing: 0: jdbc:hive2://localhost:10000/\n"
     ]
    }
   ],
   "source": [
    "! beeline -u \"jdbc:hive2://localhost:10000/\" -f db/ej3.hql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los contenidos de *db/ej3.hql* son los siguientes:\n",
    "```HiveQL\n",
    "-- Crea una vista para las tablas creadas con los siguientes atributos:\n",
    "-- Titulo, Nombre del autor, Fecha de publicación y Valoración media\n",
    "USE librosdb;\n",
    "\n",
    "CREATE VIEW IF NOT EXISTS vista_libros \n",
    "(\n",
    "    title COMMENT 'Titulo del libro',\n",
    "    author_name COMMENT 'Autor del libro',\n",
    "    publication_date COMMENT 'Fecha de publicación',\n",
    "    rating_avg COMMENT 'Valoración media del libro'\n",
    ")\n",
    "AS SELECT title, author_name, publication_date, rating_avg\n",
    "FROM authors, datasets\n",
    "WHERE authors.author_id = datasets.author_id;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ej 4. *Crea las consultas de hive para responder las siguientes cuestiones:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *¿Cuál es el título del libro que ocupa el primer lugar en el ranking de bestsellers?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scan complete in 3ms\n",
      "Connecting to jdbc:hive2://localhost:10000/\n",
      "Connected to: Apache Hive (version 1.1.0-cdh5.12.0)\n",
      "Driver: Hive JDBC (version 1.1.0-cdh5.12.0)\n",
      "Transaction isolation: TRANSACTION_REPEATABLE_READ\n",
      "de bestsellers//localhost:10000/> -- Selecciona el libro más alto en el ranking  \n",
      "0: jdbc:hive2://localhost:10000/> \n",
      "0: jdbc:hive2://localhost:10000/> USE librosdb;\n",
      "INFO  : Compiling command(queryId=hive_20201123093232_8e1beb58-8b28-491b-b8a9-760e2a7014b8): USE librosdb\n",
      "INFO  : Semantic Analysis Completed\n",
      "INFO  : Returning Hive schema: Schema(fieldSchemas:null, properties:null)\n",
      "INFO  : Completed compiling command(queryId=hive_20201123093232_8e1beb58-8b28-491b-b8a9-760e2a7014b8); Time taken: 0.054 seconds\n",
      "INFO  : Concurrency mode is disabled, not creating a lock manager\n",
      "INFO  : Executing command(queryId=hive_20201123093232_8e1beb58-8b28-491b-b8a9-760e2a7014b8): USE librosdb\n",
      "INFO  : Starting task [Stage-0:DDL] in serial mode\n",
      "INFO  : Completed executing command(queryId=hive_20201123093232_8e1beb58-8b28-491b-b8a9-760e2a7014b8); Time taken: 0.008 seconds\n",
      "INFO  : OK\n",
      "No rows affected (0.111 seconds)\n",
      "0: jdbc:hive2://localhost:10000/> \n",
      "0: jdbc:hive2://localhost:10000/> SELECT title\n",
      ". . . . . . . . . . . . . . . . > FROM dataset LEFT SEMI JOIN (\n",
      ". . . . . . . . . . . . . . . . >     SELECT min(bestsellers_rank) AS bestseller \n",
      ". . . . . . . . . . . . . . . . >     FROM dataset\n",
      ". . . . . . . . . . . . . . . . > ) AS a \n",
      ". . . . . . . . . . . . . . . . > ON bestsellers_rank = a.bestseller\n",
      ". . . . . . . . . . . . . . . . > LIMIT 1;\n",
      "Error: Error while compiling statement: FAILED: SemanticException [Error 10001]: Line 2:5 Table not found 'dataset' (state=42S02,code=10001)\n",
      "\n",
      "Closing: 0: jdbc:hive2://localhost:10000/\n"
     ]
    }
   ],
   "source": [
    "! beeline -u \"jdbc:hive2://localhost:10000/\" -f db/ej4_a.hql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El código de *db/ej4_a.hql* es el siguiente:\n",
    "```HiveQL\n",
    "-- Selecciona el libro más alto en el ranking de bestsellers\n",
    "\n",
    "USE librosdb;\n",
    "\n",
    "SELECT title\n",
    "FROM datasets LEFT SEMI JOIN (\n",
    "    SELECT max(bestsellers_rank) AS bestseller\n",
    "    FROM datasets\n",
    ") AS a \n",
    "ON bestsellers_rank = a.bestseller;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *¿Cuál es el título del libro con mejor valoración media de la autora Ana Maria Spagna?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scan complete in 3ms\n",
      "Connecting to jdbc:hive2://localhost:10000/\n",
      "Connected to: Apache Hive (version 1.1.0-cdh5.12.0)\n",
      "Driver: Hive JDBC (version 1.1.0-cdh5.12.0)\n",
      "Transaction isolation: TRANSACTION_REPEATABLE_READ\n",
      " Ana Maria Spagnaocalhost:10000/> -- Selecciona el libro con mejor valoración de \n",
      "0: jdbc:hive2://localhost:10000/> \n",
      "0: jdbc:hive2://localhost:10000/> USE librosdb;\n",
      "INFO  : Compiling command(queryId=hive_20201123093333_1ad312a5-5a01-4a0a-ad4f-b402849a93b7): USE librosdb\n",
      "INFO  : Semantic Analysis Completed\n",
      "INFO  : Returning Hive schema: Schema(fieldSchemas:null, properties:null)\n",
      "INFO  : Completed compiling command(queryId=hive_20201123093333_1ad312a5-5a01-4a0a-ad4f-b402849a93b7); Time taken: 0.05 seconds\n",
      "INFO  : Concurrency mode is disabled, not creating a lock manager\n",
      "INFO  : Executing command(queryId=hive_20201123093333_1ad312a5-5a01-4a0a-ad4f-b402849a93b7): USE librosdb\n",
      "INFO  : Starting task [Stage-0:DDL] in serial mode\n",
      "INFO  : Completed executing command(queryId=hive_20201123093333_1ad312a5-5a01-4a0a-ad4f-b402849a93b7); Time taken: 0.008 seconds\n",
      "INFO  : OK\n",
      "No rows affected (0.109 seconds)\n",
      "0: jdbc:hive2://localhost:10000/> \n",
      "0: jdbc:hive2://localhost:10000/> SELECT title\n",
      ". . . . . . . . . . . . . . . . > FROM(\n",
      ". . . . . . . . . . . . . . . . >     SELECT title, rating_avg\n",
      ". . . . . . . . . . . . . . . . >     FROM dataset LEFT SEMI JOIN (\n",
      ". . . . . . . . . . . . . . . . >         SELECT max(rating_avg) AS max_rating\n",
      ". . . . . . . . . . . . . . . . >         FROM authors, dataset\n",
      "or_id AND authors.author_name = \"Ana Maria Spagna\"thors.author_id = dataset.auth \n",
      ". . . . . . . . . . . . . . . . >     ) AS b\n",
      ". . . . . . . . . . . . . . . . >     ON rating_avg = b.max_rating\n",
      ". . . . . . . . . . . . . . . . > ) AS a\n",
      ". . . . . . . . . . . . . . . . > LIMIT 1;\n",
      "Error: Error while compiling statement: FAILED: SemanticException [Error 10001]: Line 4:5 Table not found 'dataset' (state=42S02,code=10001)\n",
      "\n",
      "Closing: 0: jdbc:hive2://localhost:10000/\n"
     ]
    }
   ],
   "source": [
    "! beeline -u \"jdbc:hive2://localhost:10000/\" -f db/ej4_b.hql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El código de *db/ej4_bb.hql* es el siguiente:\n",
    "\n",
    "```HiveQL\n",
    "-- Selecciona el libro con mejor valoración de Ana Maria Spagna\n",
    "\n",
    "USE librosdb;\n",
    "\n",
    "SELECT title\n",
    "FROM(\n",
    "    SELECT title, rating_avg\n",
    "    FROM datasets LEFT SEMI JOIN (\n",
    "        SELECT max(rating_avg) AS max_rating\n",
    "        FROM authors, datasets\n",
    "        WHERE authors.author_id = datasets.author_id AND authors.author_name = \"Ana Maria Spagna\"\n",
    "    ) AS b\n",
    "    ON rating_avg = b.max_rating\n",
    ") AS a\n",
    "LIMIT 1;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *¿Cuáles son los cicno autores que han escrito más libros?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scan complete in 3ms\n",
      "Connecting to jdbc:hive2://localhost:10000/\n",
      "Connected to: Apache Hive (version 1.1.0-cdh5.12.0)\n",
      "Driver: Hive JDBC (version 1.1.0-cdh5.12.0)\n",
      "Transaction isolation: TRANSACTION_REPEATABLE_READ\n",
      "0: jdbc:hive2://localhost:10000/> -- Selecciona los 5 autores con más libros\n",
      "0: jdbc:hive2://localhost:10000/> \n",
      "0: jdbc:hive2://localhost:10000/> USE librosdb;\n",
      "INFO  : Compiling command(queryId=hive_20201123093333_a785605f-dabf-44ff-8d55-9cfe2bbb8086): USE librosdb\n",
      "INFO  : Semantic Analysis Completed\n",
      "INFO  : Returning Hive schema: Schema(fieldSchemas:null, properties:null)\n",
      "INFO  : Completed compiling command(queryId=hive_20201123093333_a785605f-dabf-44ff-8d55-9cfe2bbb8086); Time taken: 0.072 seconds\n",
      "INFO  : Concurrency mode is disabled, not creating a lock manager\n",
      "INFO  : Executing command(queryId=hive_20201123093333_a785605f-dabf-44ff-8d55-9cfe2bbb8086): USE librosdb\n",
      "INFO  : Starting task [Stage-0:DDL] in serial mode\n",
      "INFO  : Completed executing command(queryId=hive_20201123093333_a785605f-dabf-44ff-8d55-9cfe2bbb8086); Time taken: 0.009 seconds\n",
      "INFO  : OK\n",
      "No rows affected (0.143 seconds)\n",
      "0: jdbc:hive2://localhost:10000/> \n",
      "0: jdbc:hive2://localhost:10000/> SELECT DISTINCT author_name\n",
      ". . . . . . . . . . . . . . . . > FROM authors, dataset LEFT SEMI JOIN (\n",
      "os. . . . . . . . . . . . . . . >     SELECT author_id, count(title) AS num_libr \n",
      ". . . . . . . . . . . . . . . . >     FROM dataset\n",
      ". . . . . . . . . . . . . . . . >     GROUP BY author_id\n",
      ". . . . . . . . . . . . . . . . >     ORDER BY num_libros DESC\n",
      ". . . . . . . . . . . . . . . . >     LIMIT 5\n",
      ". . . . . . . . . . . . . . . . > ) AS a\n",
      ". . . . . . . . . . . . . . . . > ON dataset.author_id = a.author_id\n",
      ". . . . . . . . . . . . . . . . > WHERE authors.author_id = dataset.author_id;\n",
      "Error: Error while compiling statement: FAILED: SemanticException [Error 10001]: Line 2:14 Table not found 'dataset' (state=42S02,code=10001)\n",
      "\n",
      "Closing: 0: jdbc:hive2://localhost:10000/\n"
     ]
    }
   ],
   "source": [
    "! beeline -u \"jdbc:hive2://localhost:10000/\" -f db/ej4_c.hql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "El código de *db/ej4_c.hql* es el siguiente:\n",
    "```HiveQL\n",
    "-- Selecciona los 5 autores con más libros\n",
    "\n",
    "USE librosdb;\n",
    "\n",
    "SELECT DISTINCT author_name\n",
    "FROM authors, datasets LEFT SEMI JOIN (\n",
    "    SELECT author_id, count(title) AS num_libros\n",
    "    FROM datasets\n",
    "    GROUP BY author_id\n",
    "    ORDER BY num_libros DESC\n",
    "    LIMIT 5\n",
    ") AS a\n",
    "ON datasets.author_id = a.author_id\n",
    "WHERE authors.author_id = datasets.author_id;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ej. 5. *Desarrolla el código MapReduce en Python que implemente la oprimera consulta del apartado anterior.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! chmod +x mapreduce/bestseller/mapper.py mapreduce/bestseller/reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! hadoop fs -put db/csv/authors.csv librosdb/authors # Al llegar a este paso los archivos desaparecen del HDFS por algun motivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "put: `librosdb/dataset/dataset.csv': File exists\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -put db/csv/dataset.csv librosdb/dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/11/23 09:33:12 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "packageJobJar: [mapreduce/bestseller/mapper.py, mapreduce/bestseller/reducer.py] [/usr/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.12.0.jar] /tmp/streamjob5246481998334469757.jar tmpDir=null\n",
      "20/11/23 09:33:14 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "20/11/23 09:33:14 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "20/11/23 09:33:14 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "20/11/23 09:33:14 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "20/11/23 09:33:14 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1604927967949_0001\n",
      "20/11/23 09:33:15 INFO impl.YarnClientImpl: Submitted application application_1604927967949_0001\n",
      "20/11/23 09:33:15 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1604927967949_0001/\n",
      "20/11/23 09:33:15 INFO mapreduce.Job: Running job: job_1604927967949_0001\n",
      "20/11/23 09:33:22 INFO mapreduce.Job: Job job_1604927967949_0001 running in uber mode : false\n",
      "20/11/23 09:33:22 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "20/11/23 09:33:29 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "20/11/23 09:33:29 INFO mapreduce.Job: Task Id : attempt_1604927967949_0001_m_000001_0, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:325)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:538)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:459)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:415)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1917)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\n",
      "\n",
      "20/11/23 09:33:34 INFO mapreduce.Job: Task Id : attempt_1604927967949_0001_m_000001_1, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:325)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:538)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:459)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:415)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1917)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\n",
      "\n",
      "20/11/23 09:33:38 INFO mapreduce.Job: Task Id : attempt_1604927967949_0001_m_000001_2, Status : FAILED\n",
      "Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:325)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:538)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:459)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:415)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1917)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\n",
      "\n",
      "20/11/23 09:33:44 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "20/11/23 09:33:44 INFO mapreduce.Job: Job job_1604927967949_0001 failed with state FAILED due to: Task failed task_1604927967949_0001_m_000001\n",
      "Job failed as tasks failed. failedMaps:1 failedReduces:0\n",
      "\n",
      "20/11/23 09:33:45 INFO mapreduce.Job: Counters: 39\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=0\n",
      "\t\tFILE: Number of bytes written=128668\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=1699749\n",
      "\t\tHDFS: Number of bytes written=0\n",
      "\t\tHDFS: Number of read operations=3\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=0\n",
      "\tJob Counters \n",
      "\t\tFailed map tasks=4\n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=5\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tOther local map tasks=3\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=14945\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=11468\n",
      "\t\tTotal time spent by all map tasks (ms)=14945\n",
      "\t\tTotal time spent by all reduce tasks (ms)=11468\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=14945\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=11468\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=15303680\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=11743232\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=18922\n",
      "\t\tMap output records=1\n",
      "\t\tMap output bytes=49\n",
      "\t\tMap output materialized bytes=57\n",
      "\t\tInput split bytes=126\n",
      "\t\tCombine input records=0\n",
      "\t\tSpilled Records=1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=27\n",
      "\t\tCPU time spent (ms)=830\n",
      "\t\tPhysical memory (bytes) snapshot=298876928\n",
      "\t\tVirtual memory (bytes) snapshot=1569234944\n",
      "\t\tTotal committed heap usage (bytes)=326631424\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=1699623\n",
      "20/11/23 09:33:45 ERROR streaming.StreamJob: Job not successful!\n",
      "Streaming Command Failed!\n"
     ]
    }
   ],
   "source": [
    "! hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\\\n",
    "-file mapreduce/bestseller/mapper.py \\\\\n",
    "-mapper ./mapreduce/bestseller/mapper.py \\\\\n",
    "-file mapreduce/bestseller/reducer.py \\\\\n",
    "-reducer ./mapreduce/bestseller/reducer.py \\\\\n",
    "-input /user/cloudera/librosdb/dataset/* \\\\\n",
    "-output /user/cloudera/librosdb/ej5-mapreduce-output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atlas Obscura, 2nd Edition : An Explorer's Guide to the World's Hidden Wonders\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "cat db/csv/dataset.csv | mapreduce/bestseller/mapper.py | sort | mapreduce/bestseller/reducer.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
